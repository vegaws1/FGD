import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from scipy.special import binom

# ========== HYPERPARAMETERS ========== 
ALPHA = 0.6    # Fractional order (initialize upfront)
LAMBDA = 0.5   # Tempering parameter (initialize upfront)
LEARNING_RATE = 0.1
EPOCHS = 100

# Load and Preprocess Data
data = load_breast_cancer()
X, y = data.data, data.target
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Sigmoid and Loss
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cross_entropy_loss(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))

# TFGD Optimizer (Properly uses α and λ)
class TFGDOptimizer:
    def __init__(self):
        self.S = None  # Memory term
    
    def update(self, params, grads):
        if self.S is None:
            self.S = np.zeros_like(params)
        # Update rule: S_k = binom(α,0)*grads + e^{-λ} * S_{k-1}
        self.S = binom(ALPHA, 0) * grads + np.exp(-LAMBDA) * self.S  # binom(α,0) = 1
        params -= LEARNING_RATE * self.S
        return params

# Logistic Regression Model
class LogisticRegression:
    def __init__(self, input_dim):
        self.weights = np.random.randn(input_dim)
        self.bias = np.random.randn(1)
    
    def forward(self, X):
        return sigmoid(X @ self.weights + self.bias)
    
    def compute_gradients(self, X, y_true, y_pred):
        error = y_pred - y_true
        dw = (X.T @ error) / len(y_true)
        db = np.mean(error)
        return dw, db

# Training with TFGD
def train_tfgd(X_train, y_train):
    model = LogisticRegression(X_train.shape[1])
    optimizer_weights = TFGDOptimizer()
    optimizer_bias = TFGDOptimizer()
    losses = []
    
    for epoch in range(EPOCHS):
        y_pred = model.forward(X_train)
        loss = cross_entropy_loss(y_train, y_pred)
        losses.append(loss)
        
        dw, db = model.compute_gradients(X_train, y_train, y_pred)
        model.weights = optimizer_weights.update(model.weights, dw)
        model.bias = optimizer_bias.update(model.bias, db)
        
    return model, losses

# Training with SGD (Baseline)
def train_sgd(X_train, y_train):
    model = LogisticRegression(X_train.shape[1])
    losses = []
    
    for epoch in range(EPOCHS):
        y_pred = model.forward(X_train)
        loss = cross_entropy_loss(y_train, y_pred)
        losses.append(loss)
        
        dw, db = model.compute_gradients(X_train, y_train, y_pred)
        model.weights -= LEARNING_RATE * dw
        model.bias -= LEARNING_RATE * db
        
    return model, losses

# Train Models
model_tfgd, loss_tfgd = train_tfgd(X_train, y_train)
model_sgd, loss_sgd = train_sgd(X_train, y_train)

# Evaluate
y_pred_tfgd = (model_tfgd.forward(X_test) > 0.5).astype(int)
y_pred_sgd = (model_sgd.forward(X_test) > 0.5).astype(int)

print(f"TFGD (α={ALPHA}, λ={LAMBDA}) Test Accuracy: {accuracy_score(y_test, y_pred_tfgd):.4f}")
print(f"SGD Test Accuracy: {accuracy_score(y_test, y_pred_sgd):.4f}")

# Plot
plt.figure(figsize=(10, 5))
plt.plot(loss_tfgd, label=f'TFGD (α={ALPHA}, λ={LAMBDA})', linestyle='--')
plt.plot(loss_sgd, label='SGD', linestyle='-')
plt.xlabel('Epochs')
plt.ylabel('Cross-Entropy Loss')
plt.legend()
plt.show()


#-------Sensitivity Analysis on Hyperparameters---------

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from scipy.special import binom

# ========== SETTINGS ==========
ALPHA_LIST = np.linspace(0.1, 1.0, 10)   # values of fractional order to try
LAMBDA_LIST = np.linspace(0.0, 1.0, 10)  # values of tempering to try
LEARNING_RATE = 0.1
MAX_EPOCHS = 200
CONV_THRESHOLD = 1e-4   # threshold for loss change to count as "converged"
CONV_PATIENCE = 5       # number of epochs with small change to trigger convergence

# Load and preprocess data
data = load_breast_cancer()
X, y = data.data, data.target
X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Activation and loss
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cross_entropy_loss(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-8) +
                    (1 - y_true) * np.log(1 - y_pred + 1e-8))

# TFGD optimizer that uses α and λ
class TFGDOptimizer:
    def __init__(self, alpha, lambd):
        self.alpha = alpha
        self.lambd = lambd
        self.S = None

    def update(self, params, grads):
        if self.S is None:
            self.S = np.zeros_like(params)
        # S_k = binom(α,0)*grads + exp(-λ)*S_{k-1}, and binom(α,0)=1
        self.S = grads + np.exp(-self.lambd) * self.S
        return params - LEARNING_RATE * self.S

# Simple logistic regression model
class LogisticRegression:
    def __init__(self, dim):
        self.w = np.random.randn(dim)
        self.b = np.random.randn()

    def forward(self, X):
        return sigmoid(X @ self.w + self.b)

    def grads(self, X, y_true, y_pred):
        err = y_pred - y_true
        dw = (X.T @ err) / len(y_true)
        db = np.mean(err)
        return dw, db

# Train with TFGD, returning final accuracy and epoch of convergence
def train_and_evaluate(alpha, lambd):
    model = LogisticRegression(X_train.shape[1])
    opt_w = TFGDOptimizer(alpha, lambd)
    opt_b = TFGDOptimizer(alpha, lambd)
    losses = []

    for epoch in range(1, MAX_EPOCHS + 1):
        y_pred = model.forward(X_train)
        loss = cross_entropy_loss(y_train, y_pred)
        losses.append(loss)

        dw, db = model.grads(X_train, y_train, y_pred)
        model.w = opt_w.update(model.w, dw)
        model.b = opt_b.update(model.b, db)

        # check convergence: small change over last CONV_PATIENCE epochs
        if epoch > CONV_PATIENCE:
            recent = np.diff(losses[-CONV_PATIENCE:])
            if np.all(np.abs(recent) < CONV_THRESHOLD):
                conv_epoch = epoch
                break
    else:
        conv_epoch = MAX_EPOCHS

    # test accuracy
    y_test_pred = (model.forward(X_test) > 0.5).astype(int)
    acc = accuracy_score(y_test, y_test_pred)
    return acc, conv_epoch

# Preallocate results
acc_vs_alpha = []
conv_vs_alpha = []
for α in ALPHA_LIST:
    acc, epoch = train_and_evaluate(alpha=α, lambd=0.5)
    acc_vs_alpha.append(acc)
    conv_vs_alpha.append(epoch)

acc_vs_lambda = []
conv_vs_lambda = []
for λ in LAMBDA_LIST:
    acc, epoch = train_and_evaluate(alpha=0.6, lambd=λ)
    acc_vs_lambda.append(acc)
    conv_vs_lambda.append(epoch)

# === Plot the 4-panel sensitivity figure ===
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
plt.subplots_adjust(wspace=0.3, hspace=0.4)

# (a) Accuracy vs α (λ fixed)
axes[0,0].plot(ALPHA_LIST, acc_vs_alpha, marker='o')
axes[0,0].set_title('(a) Accuracy vs. α\n(λ = 0.5)')
axes[0,0].set_xlabel('α')
axes[0,0].set_ylabel('Test Accuracy')

# (b) Accuracy vs λ (α fixed)
axes[0,1].plot(LAMBDA_LIST, acc_vs_lambda, marker='o')
axes[0,1].set_title('(b) Accuracy vs. λ\n(α = 0.6)')
axes[0,1].set_xlabel('λ')
axes[0,1].set_ylabel('Test Accuracy')

# (c) Convergence epoch vs α
axes[1,0].plot(ALPHA_LIST, conv_vs_alpha, marker='o')
axes[1,0].set_title('(c) Convergence Epoch vs. α')
axes[1,0].set_xlabel('α')
axes[1,0].set_ylabel('Epoch of Convergence')

# (d) Convergence epoch vs λ
axes[1,1].plot(LAMBDA_LIST, conv_vs_lambda, marker='o')
axes[1,1].set_title('(d) Convergence Epoch vs. λ')
axes[1,1].set_xlabel('λ')
axes[1,1].set_ylabel('Epoch of Convergence')

fig.suptitle('Sensitivity analysis', fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig('images/sensitivity.png', dpi=300)
plt.show()

#------------Additional Benchmark Experiments-------------

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer, make_classification, load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from scipy.special import binom

# ========== HYPERPARAMETERS ==========
# Breast‑cancer sweep
ALPHA_LIST = np.linspace(0.1, 1.0, 10)
LAMBDA_LIST = np.linspace(0.0, 1.0, 10)
LEARNING_RATE = 0.1
MAX_EPOCHS = 200
CONV_THRESHOLD = 1e-4
CONV_PATIENCE = 5

# Extra benchmarks
EPOCHS_EXTRA = 100  # you can reduce for speed
BEST_ALPHA = 0.7
BEST_LAMBDA = 0.3

# ========== HELPERS ==========

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cross_entropy_loss(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-8) +
                    (1 - y_true) * np.log(1 - y_pred + 1e-8))

class TFGDOptimizer:
    def __init__(self, alpha, lambd):
        self.alpha = alpha
        self.lambd = lambd
        self.S = None

    def update(self, params, grads):
        if self.S is None:
            self.S = np.zeros_like(params)
        # binom(alpha,0) = 1
        self.S = grads + np.exp(-self.lambd) * self.S
        return params - LEARNING_RATE * self.S

class LogisticRegression:
    def __init__(self, dim):
        self.w = np.random.randn(dim)
        self.b = np.random.randn()

    def forward(self, X):
        return sigmoid(X @ self.w + self.b)

    def compute_gradients(self, X, y_true, y_pred):
        err = y_pred - y_true
        dw = (X.T @ err) / len(y_true)
        db = np.mean(err)
        return dw, db

# ========== SENSITIVITY SWEEP ==========

def train_and_get_metrics(alpha, lambd):
    # load & preprocess
    data = load_breast_cancer()
    X, y = data.data, data.target
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    model = LogisticRegression(X_train.shape[1])
    opt_w = TFGDOptimizer(alpha, lambd)
    opt_b = TFGDOptimizer(alpha, lambd)
    losses = []

    for epoch in range(1, MAX_EPOCHS + 1):
        y_pred = model.forward(X_train)
        loss = cross_entropy_loss(y_train, y_pred)
        losses.append(loss)

        dw, db = model.compute_gradients(X_train, y_train, y_pred)
        model.w = opt_w.update(model.w, dw)
        model.b = opt_b.update(model.b, db)

        if epoch > CONV_PATIENCE:
            recent = np.diff(losses[-CONV_PATIENCE:])
            if np.all(np.abs(recent) < CONV_THRESHOLD):
                conv_epoch = epoch
                break
    else:
        conv_epoch = MAX_EPOCHS

    y_test_pred = (model.forward(X_test) > 0.5).astype(int)
    acc = accuracy_score(y_test, y_test_pred)
    return acc, conv_epoch

# Run sweeps
acc_vs_alpha, conv_vs_alpha = [], []
for α in ALPHA_LIST:
    a, c = train_and_get_metrics(α, 0.5)
    acc_vs_alpha.append(a)
    conv_vs_alpha.append(c)

acc_vs_lambda, conv_vs_lambda = [], []
for λ in LAMBDA_LIST:
    a, c = train_and_get_metrics(0.6, λ)
    acc_vs_lambda.append(a)
    conv_vs_lambda.append(c)

# Plot sensitivity
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
plt.subplots_adjust(wspace=0.3, hspace=0.4)

axes[0,0].plot(ALPHA_LIST, acc_vs_alpha, marker='o')
axes[0,0].set_title('(a) Accuracy vs. α (λ=0.5)')
axes[0,0].set_xlabel('α'); axes[0,0].set_ylabel('Test Accuracy')

axes[0,1].plot(LAMBDA_LIST, acc_vs_lambda, marker='o')
axes[0,1].set_title('(b) Accuracy vs. λ (α=0.6)')
axes[0,1].set_xlabel('λ'); axes[0,1].set_ylabel('Test Accuracy')

axes[1,0].plot(ALPHA_LIST, conv_vs_alpha, marker='o')
axes[1,0].set_title('(c) Convergence Epoch vs. α')
axes[1,0].set_xlabel('α'); axes[1,0].set_ylabel('Epoch of Convergence')

axes[1,1].plot(LAMBDA_LIST, conv_vs_lambda, marker='o')
axes[1,1].set_title('(d) Convergence Epoch vs. λ')
axes[1,1].set_xlabel('λ'); axes[1,1].set_ylabel('Epoch of Convergence')

fig.suptitle('Sensitivity analysis', fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.savefig('images/sensitivity.png', dpi=300)
plt.show()

# ========== EXTRA BENCHMARKS ==========

def evaluate_on_dataset(X, y, alpha, lambd, epochs):
    # preprocess
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    # TFGD
    model = LogisticRegression(X_train.shape[1])
    opt_w = TFGDOptimizer(alpha, lambd)
    opt_b = TFGDOptimizer(alpha, lambd)
    for _ in range(epochs):
        y_pred = model.forward(X_train)
        dw, db = model.compute_gradients(X_train, y_train, y_pred)
        model.w = opt_w.update(model.w, dw)
        model.b = opt_b.update(model.b, db)
    acc_tfgd = accuracy_score(
        y_test, (model.forward(X_test) > 0.5).astype(int))

    # SGD
    model_s = LogisticRegression(X_train.shape[1])
    for _ in range(epochs):
        y_pred_s = model_s.forward(X_train)
        dw, db = model_s.compute_gradients(X_train, y_train, y_pred_s)
        model_s.w -= LEARNING_RATE * dw
        model_s.b -= LEARNING_RATE * db
    acc_sgd = accuracy_score(
        y_test, (model_s.forward(X_test) > 0.5).astype(int))

    return acc_tfgd, acc_sgd

# 1) High‑dim synthetic
X_hd, y_hd = make_classification(
    n_samples=2000, n_features=1000,
    n_informative=50, n_redundant=0, n_repeated=0,
    n_classes=2, flip_y=0.01, class_sep=1.0,
    random_state=42)

# 2) Noisy digits (0 vs rest)
digits = load_digits()
X_d, y_d = digits.data, digits.target
y_d = (y_d == 0).astype(int)
noise = np.random.normal(scale=4.0, size=X_d.shape)
X_dn = X_d + noise

for name, X_, y_ in [
    ("High‑dim synthetic", X_hd, y_hd),
    ("Noisy digits (0 vs rest)", X_dn, y_d)
]:
    acc_t, acc_s = evaluate_on_dataset(
        X_, y_, BEST_ALPHA, BEST_LAMBDA, EPOCHS_EXTRA)
    print(f"{name:25s} | TFGD: {acc_t:.4f} | SGD: {acc_s:.4f}")

#----------------------------------Comparison with Adaptive Optimizers

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from scipy.special import binom

# Ensure the output directory exists
os.makedirs('figures', exist_ok=True)

# Hyperparameters
ALPHA = 0.6    # Fractional order for TFGD
LAMBDA = 0.5   # Tempering parameter for TFGD
LEARNING_RATE = 0.1
EPOCHS = 100

# Load and preprocess data
data = load_breast_cancer()
X, y = data.data, data.target
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Sigmoid and loss functions
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cross_entropy_loss(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))

# Logistic Regression model
class LogisticRegression:
    def __init__(self, dim):
        self.w = np.random.randn(dim)
        self.b = np.random.randn()
        
    def forward(self, X):
        return sigmoid(X @ self.w + self.b)
    
    def grads(self, X, y_true, y_pred):
        err = y_pred - y_true
        dw = (X.T @ err) / len(y_true)
        db = np.mean(err)
        return dw, db

# TFGD Optimizer
class TFGDOptimizer:
    def __init__(self, alpha, lambd):
        self.alpha = alpha
        self.lambd = lambd
        self.S = None

    def update(self, params, grads):
        if self.S is None:
            self.S = np.zeros_like(params)
        self.S = binom(self.alpha, 0) * grads + np.exp(-self.lambd) * self.S
        return params - LEARNING_RATE * self.S

# Adam Optimizer
class AdamOptimizer:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = None
        self.v = None
        self.t = 0

    def update(self, params, grads):
        if self.m is None:
            self.m = np.zeros_like(grads)
            self.v = np.zeros_like(grads)
        self.t += 1
        self.m = self.beta1 * self.m + (1 - self.beta1) * grads
        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)
        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)

# RMSprop Optimizer
class RMSpropOptimizer:
    def __init__(self, lr=0.001, decay=0.9, eps=1e-8):
        self.lr = lr
        self.decay = decay
        self.eps = eps
        self.cache = None

    def update(self, params, grads):
        if self.cache is None:
            self.cache = np.zeros_like(grads)
        self.cache = self.decay * self.cache + (1 - self.decay) * (grads ** 2)
        return params - self.lr * grads / (np.sqrt(self.cache) + self.eps)

# Training function
def train_model(optimizer_class, optimizer_kwargs):
    model = LogisticRegression(X_train.shape[1])
    opt_w = optimizer_class(**optimizer_kwargs)
    opt_b = optimizer_class(**optimizer_kwargs)
    losses = []

    for epoch in range(EPOCHS):
        y_pred = model.forward(X_train)
        loss = cross_entropy_loss(y_train, y_pred)
        losses.append(loss)

        dw, db = model.grads(X_train, y_train, y_pred)
        model.w = opt_w.update(model.w, dw)
        model.b = opt_b.update(model.b, db)

    # Final test accuracy
    y_test_pred = (model.forward(X_test) > 0.5).astype(int)
    acc = accuracy_score(y_test, y_test_pred)
    return losses, acc

# Run experiments
loss_tfgd, acc_tfgd = train_model(TFGDOptimizer, {'alpha': ALPHA, 'lambd': LAMBDA})
loss_adam, acc_adam = train_model(AdamOptimizer, {'lr': 0.001})
loss_rms, acc_rms = train_model(RMSpropOptimizer, {'lr': 0.001, 'decay': 0.9})

# Print results
print(f"TFGD (α={ALPHA}, λ={LAMBDA}) Test Accuracy: {acc_tfgd:.4f}")
print(f"Adam Test Accuracy: {acc_adam:.4f}")
print(f"RMSprop Test Accuracy: {acc_rms:.4f}")

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(loss_tfgd, label='TFGD', linestyle='--')
plt.plot(loss_adam, label='Adam', linestyle='-')
plt.plot(loss_rms, label='RMSprop', linestyle=':')
plt.xlabel('Epochs')
plt.ylabel('Cross-Entropy Loss')
plt.legend()
plt.tight_layout()
plt.savefig('figures/adaptive_curves.png', dpi=300)
plt.show()

#-------------------Non Convex analysis

import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist

# Ensure the output directory exists
os.makedirs('figures', exist_ok=True)

# Load MNIST data
(x_train, _), _ = mnist.load_data()
x_train = x_train.astype('float32') / 255.0
x_train = x_train.reshape((x_train.shape[0], -1))  # flatten to (60000, 784)

# Hyperparameters
input_dim = x_train.shape[1]
hidden_dim = 128
epochs = 50
learning_rate = 0.01

# Activation and loss functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_deriv(x):
    s = sigmoid(x)
    return s * (1 - s)

def mse_loss(x_true, x_pred):
    return np.mean((x_true - x_pred) ** 2)

# Optimizers
class TFGDOptimizer:
    def __init__(self, alpha, lambd):
        self.alpha = alpha
        self.lambd = lambd
        self.S = None

    def update(self, params, grads):
        if self.S is None:
            self.S = np.zeros_like(params)
        self.S = grads + np.exp(-self.lambd) * self.S
        return params - learning_rate * self.S

class AdamOptimizer:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = None
        self.v = None
        self.t = 0

    def update(self, params, grads):
        if self.m is None:
            self.m = np.zeros_like(grads)
            self.v = np.zeros_like(grads)
        self.t += 1
        self.m = self.beta1 * self.m + (1 - self.beta1) * grads
        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)
        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)

# Training function for autoencoder
def train_autoencoder(optimizer_class, opt_kwargs):
    # Initialize weights and biases
    W1 = np.random.randn(input_dim, hidden_dim) * 0.01
    b1 = np.zeros(hidden_dim)
    W2 = np.random.randn(hidden_dim, input_dim) * 0.01
    b2 = np.zeros(input_dim)
    
    # Initialize optimizers
    opt_W1 = optimizer_class(**opt_kwargs)
    opt_b1 = optimizer_class(**opt_kwargs)
    opt_W2 = optimizer_class(**opt_kwargs)
    opt_b2 = optimizer_class(**opt_kwargs)
    
    losses = []
    for epoch in range(1, epochs + 1):
        # Forward pass
        z1 = x_train.dot(W1) + b1
        h = sigmoid(z1)
        z2 = h.dot(W2) + b2
        x_rec = sigmoid(z2)
        
        # Compute loss
        loss = mse_loss(x_train, x_rec)
        losses.append(loss)
        
        # Backpropagation
        d_rec = (x_rec - x_train) * sigmoid_deriv(z2) / x_train.shape[0]
        dW2 = h.T.dot(d_rec)
        db2 = np.sum(d_rec, axis=0)
        
        dh = d_rec.dot(W2.T) * sigmoid_deriv(z1)
        dW1 = x_train.T.dot(dh)
        db1 = np.sum(dh, axis=0)
        
        # Update parameters
        W2 = opt_W2.update(W2, dW2)
        b2 = opt_b2.update(b2, db2)
        W1 = opt_W1.update(W1, dW1)
        b1 = opt_b1.update(b1, db1)
    
    return losses

# Run training
loss_tfgd = train_autoencoder(TFGDOptimizer, {'alpha': 0.6, 'lambd': 0.5})
loss_adam = train_autoencoder(AdamOptimizer, {'lr': 0.001})

# Print final losses
print(f"Final reconstruction loss (TFGD): {loss_tfgd[-1]:.6f}")
print(f"Final reconstruction loss (Adam): {loss_adam[-1]:.6f}")

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(loss_tfgd, label='TFGD', linestyle='--')
plt.plot(loss_adam, label='Adam', linestyle='-')
plt.xlabel('Epochs')
plt.ylabel('MSE Reconstruction Loss')
plt.legend()
plt.tight_layout()
plt.savefig('figures/nonconvex_mnist.png', dpi=300)
plt.show()




