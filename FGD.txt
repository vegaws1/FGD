import numpy as np
import tensorflow as tf
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from scipy.special import gamma
import matplotlib.pyplot as plt

# Load and preprocess Breast Cancer dataset
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reshape y for binary classification compatibility with softmax (2 classes)
y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)
y_train_onehot = np.hstack([1 - y_train, y_train])  # One-hot encoding: [0, 1] or [1, 0]
y_test_onehot = np.hstack([1 - y_test, y_test])

# Define the model: deeper network to increase optimization complexity
model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(30,)),  # 30 features
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')  # 2 classes
])

# Hyperparameters
learning_rate = 0.5
alpha = 0.1  # Fractional order for FGD
history_length = 10  # Number of past gradients to consider
num_epochs = 300

# Loss function
loss_fn = tf.keras.losses.CategoricalCrossentropy()

# Function to compute Grünwald-Letnikov weights for FGD
def compute_gl_weights(k, alpha):
    weights = []
    for j in range(k + 1):
        # Add small epsilon to avoid division issues in gamma
        binom = (-1)**j * gamma(alpha + 1) / (gamma(j + 1) * gamma(alpha - j + 1 + 1e-10))
        weights.append(binom)
    return np.array(weights)

# Training function for GD
def train_gd(model, X_train, y_train_onehot, learning_rate, num_epochs):
    losses = []
    for epoch in range(num_epochs):
        with tf.GradientTape() as tape:
            preds = model(X_train, training=True)
            loss = loss_fn(y_train_onehot, preds)
        grads = tape.gradient(loss, model.trainable_variables)
        for var, grad in zip(model.trainable_variables, grads):
            var.assign_sub(learning_rate * grad)
        losses.append(loss.numpy())
    return losses

# Training function for FGD
def train_fgd(model, X_train, y_train_onehot, learning_rate, alpha, history_length, num_epochs):
    grad_history = [[] for _ in model.trainable_variables]
    losses = []
    for epoch in range(num_epochs):
        with tf.GradientTape() as tape:
            preds = model(X_train, training=True)
            loss = loss_fn(y_train_onehot, preds)
        grads = tape.gradient(loss, model.trainable_variables)
        # Update gradient history
        for i, grad in enumerate(grads):
            grad_history[i].append(grad)
            if len(grad_history[i]) > history_length:
                grad_history[i].pop(0)
        # Compute fractional gradients
        k = len(grad_history[0]) - 1  # Number of past gradients available
        weights = compute_gl_weights(k, alpha)
        fractional_grads = []
        for i in range(len(grads)):
            f_grad = sum(weights[j] * grad_history[i][-(j + 1)] for j in range(k + 1))
            fractional_grads.append(f_grad)
        # Update parameters
        for var, f_grad in zip(model.trainable_variables, fractional_grads):
            var.assign_sub(learning_rate * f_grad)
        losses.append(loss.numpy())
    return losses

# Train with GD
model_gd = tf.keras.models.clone_model(model)
model_gd.compile(optimizer='sgd', loss=loss_fn)  # Compile to initialize weights
gd_losses = train_gd(model_gd, X_train, y_train_onehot, learning_rate, num_epochs)

# Train with FGD
model_fgd = tf.keras.models.clone_model(model)
model_fgd.compile(optimizer='sgd', loss=loss_fn)  # Compile to initialize weights
fgd_losses = train_fgd(model_fgd, X_train, y_train_onehot, learning_rate, alpha, history_length, num_epochs)

# Plot training losses
plt.figure(figsize=(10, 5))
plt.plot(gd_losses, label='GD')
plt.plot(fgd_losses, label=f'FGD (α={alpha})')
plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.legend()
plt.title('Training Loss: GD vs FGD on Breast Cancer Dataset')
plt.show()

# Optional: Evaluate final accuracy
gd_preds = model_gd.predict(X_test)
fgd_preds = model_fgd.predict(X_test)
gd_accuracy = np.mean(np.argmax(gd_preds, axis=1) == np.argmax(y_test_onehot, axis=1))
fgd_accuracy = np.mean(np.argmax(fgd_preds, axis=1) == np.argmax(y_test_onehot, axis=1))
print(f"GD Test Accuracy: {gd_accuracy:.4f}")
print(f"FGD Test Accuracy: {fgd_accuracy:.4f}")
