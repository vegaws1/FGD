import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from scipy.special import gamma
import matplotlib.pyplot as plt

# Load and preprocess MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(-1, 28*28).astype("float32") / 255.0
x_test = x_test.reshape(-1, 28*28).astype("float32") / 255.0
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Model definition
def create_model():
    model = Sequential([
        Dense(128, activation='relu', input_shape=(784,), 
              kernel_initializer='he_normal', bias_initializer='zeros'),
        Dense(10, activation='softmax', 
              kernel_initializer='glorot_uniform', bias_initializer='zeros')
    ])
    return model

# Classical GD (Baseline)
model_gd = create_model()
model_gd.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),  # Increased LR
                 loss='categorical_crossentropy', 
                 metrics=['accuracy'])
history_gd = model_gd.fit(x_train, y_train, 
                          epochs=20,  # Fewer epochs for demonstration
                          batch_size=256, 
                          validation_data=(x_test, y_test), 
                          verbose=1)

# FGD Implementation (Aligned with Theory)
alpha = 0.7  # Adjusted for better performance
eta = 0.05   # Increased learning rate
epochs = 20
batch_size = 256
history_length = 20  # Limit gradient memory

def compute_gl_weights(k, alpha):
    weights = []
    for j in range(k + 1):
        binom = (-1)**j * gamma(alpha + 1) / (gamma(j + 1) * gamma(alpha - j + 1 + 1e-10))
        weights.append(binom)
    return np.array(weights) / gamma(1 - alpha)  # Normalization

model_fgd = create_model()
grad_history = []
train_loss_fgd = []
val_accuracy_fgd = []

for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")
    for i in range(0, len(x_train), batch_size):
        x_batch = x_train[i:i+batch_size]
        y_batch = y_train[i:i+batch_size]
        
        with tf.GradientTape() as tape:
            preds = model_fgd(x_batch)
            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_batch, preds))
        grads = tape.gradient(loss, model_fgd.trainable_variables)
        grad_history.append(grads)
        
        # Truncate history to avoid memory overload
        if len(grad_history) > history_length:
            grad_history.pop(0)
        
        k = len(grad_history) - 1
        weights = compute_gl_weights(k, alpha)
        fractional_grad = [tf.zeros_like(g) for g in grads]
        
        for j in range(k + 1):
            for var_idx in range(len(grads)):
                fractional_grad[var_idx] += weights[j] * grad_history[-(j+1)][var_idx]
        
        # Update with constant learning rate
        for var, f_grad in zip(model_fgd.trainable_variables, fractional_grad):
            var.assign_sub(eta * f_grad)
    
    # Track metrics
    train_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_train, model_fgd(x_train))).numpy()
    train_loss_fgd.append(train_loss)
    val_pred = model_fgd.predict(x_test, verbose=0)
    val_accuracy = np.mean(np.argmax(val_pred, axis=1) == np.argmax(y_test, axis=1))
    val_accuracy_fgd.append(val_accuracy)

# Plot results
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history_gd.history['loss'], label='GD Loss')
plt.plot(train_loss_fgd, label=f'FGD (α={alpha})')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_gd.history['val_accuracy'], label='GD Accuracy')
plt.plot(val_accuracy_fgd, label=f'FGD (α={alpha})')
plt.title('Test Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()